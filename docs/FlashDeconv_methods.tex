\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{geometry}
\geometry{margin=1in}

\title{FlashDeconv: Fast Linear Algebra for Scalable Hybrid Deconvolution}
\author{}
\date{}

\begin{document}

\maketitle

\section*{Introduction}

This is a fully evolved methodological framework. Based on reflections on Cell2location (accuracy), RCTD (platform effects), and CARD (spatial correlation), this new version addresses the shortcomings of \textbf{non-Gaussian distribution handling}, \textbf{platform effect correction}, and \textbf{rare cell type preservation}, while maintaining the extreme speed brought by \textbf{randomized sketching}.

\section{Methods: FlashDeconv}

\subsection{Overview and Statistical Framework}

The fundamental challenge in spatial transcriptomics (ST) deconvolution is solving the cell type abundance matrix $\beta \in \mathbb{R}^{N \times K}$ given the spatial gene expression matrix $Y \in \mathbb{R}^{N \times G}$ and the single-cell reference signature matrix $X \in \mathbb{R}^{K \times G}$, such that $Y \approx \beta X$. Current Bayesian methods (e.g., Cell2location) model the raw counts using Negative Binomial distributions, which provides high accuracy but incurs prohibitive computational costs for million-scale datasets. Conversely, fast linear regression methods (e.g., NNLS) often ignore the heteroscedastic noise of count data and platform-specific effects.

\textbf{FlashDeconv} introduces a four-stage framework that combines:
\begin{enumerate}
    \item \textbf{Gene Selection} to identify informative genes (highly variable genes + cell-type markers).
    \item \textbf{Data Preprocessing} with flexible normalization options (log-CPM, Pearson residuals, or raw).
    \item \textbf{Structure-Preserving Matrix Sketching} to compress genomic dimensions ($G \approx 20,000$) into a biologically informative low-dimensional subspace ($d \approx 500$), reducing memory usage by orders of magnitude.
    \item \textbf{Spatial Graph Regularized Optimization} to explicitly model spatial autocorrelation among neighboring spots, solved via a high-performance Coordinate Descent algorithm.
\end{enumerate}

\subsection{Data Preprocessing}

Raw ST data follows a Poisson-Gamma mixture distribution where variance depends on the mean. Direct application of Euclidean-based sketching (Johnson-Lindenstrauss lemma) on raw counts is statistically invalid. FlashDeconv provides three preprocessing strategies, selectable via the \texttt{preprocess} parameter:

\subsubsection{Method 1: Log-CPM Normalization (Default)}

The default and recommended preprocessing (\texttt{preprocess="log\_cpm"}) normalizes counts to Counts Per Million (CPM) and applies log1p transformation:
\begin{equation}
\tilde{Y}_{ig} = \log(1 + 10^4 \cdot \frac{Y_{ig}}{N_i}), \quad \tilde{X}_{kg} = \log(1 + 10^4 \cdot \frac{X_{kg}}{N_k^{ref}})
\end{equation}
where $N_i = \sum_g Y_{ig}$ is the total count for spot $i$ and $N_k^{ref} = \sum_g X_{kg}$ is the total count for cell type $k$. This normalization:
\begin{itemize}
    \item Removes sequencing depth effects
    \item Stabilizes variance via log transformation
    \item Preserves non-negativity for downstream NNLS
\end{itemize}

\subsubsection{Method 2: Uncentered Pearson Residuals}

For variance-stabilizing transformation (\texttt{preprocess="pearson"}), we use \textbf{uncentered} Pearson residuals that divide by the expected standard deviation:
\begin{equation}
\tilde{Y}_{ig} = \frac{Y_{ig}}{\sigma_g^Y}, \quad \tilde{X}_{kg} = \frac{X_{kg}}{\sigma_g^X}
\end{equation}
where the gene-wise standard deviations are computed using a Negative Binomial variance model:
\begin{equation}
\sigma_g^Y = \sqrt{\bar{\mu}_g^Y + (\bar{\mu}_g^Y)^2 / \theta}, \quad \sigma_g^X = \sqrt{\bar{\mu}_g^X + (\bar{\mu}_g^X)^2 / \theta}
\end{equation}
Here $\bar{\mu}_g^Y = \text{mean}_i(Y_{ig})$ and $\bar{\mu}_g^X = \text{mean}_k(X_{kg})$ are the gene-wise means, and $\theta = 100$ is a fixed overdispersion parameter.

\textbf{Key Design Choice:} Unlike standard Pearson residuals $(Y - \mu)/\sigma$, we use uncentered residuals $Y/\sigma$ to preserve non-negativity. This is essential because downstream solving uses non-negative least squares (NNLS), which requires non-negative inputs.

\subsubsection{Method 3: Raw Counts}

For pre-normalized data or synthetic experiments (\texttt{preprocess="raw"}), the raw counts can be used directly:
\begin{equation}
\tilde{Y} = Y, \quad \tilde{X} = X
\end{equation}

\textbf{Recommendation:} Use \texttt{log\_cpm} (default) for most real datasets. Use \texttt{pearson} for data with high technical noise. Use \texttt{raw} only for pre-normalized or synthetic data.

\subsection{Structure-Preserving Randomized Sketching}

Solving the regression problem in the original $G$-dimensional space ($G \approx 20,000$) is computationally wasteful due to the high collinearity of gene expression. We propose \textbf{Structure-Preserving Sketching}, an improvement over standard random projections.

We define a sketching matrix $\Omega \in \mathbb{R}^{G \times d}$, where $d \ll G$ (default $d=512$). Instead of a dense Gaussian matrix (which is slow to generate and multiply), we use a \textbf{Sparse CountSketch Matrix} with an \textbf{Importance Sampling} twist:

\begin{enumerate}
    \item \textbf{Feature Selection:} We select a subset of Informative Genes $\mathcal{G}_{info}$ (Union of Highly Variable Genes and Cell-type Specific Markers) to prevent noise accumulation.
    \item \textbf{Weighted Sketching:} For each column $j \in \{1 \dots d\}$ of $\Omega$, we assign exactly one non-zero value $\pm 1$ for each gene $g$, but the probability of assignment is weighted by the gene's \textbf{Leverage Score} (biological importance). This ensures that marker genes for rare cell types are preserved with high probability.
\end{enumerate}

\subsubsection{CountSketch Matrix Construction (Detailed Algorithm)}

The CountSketch matrix $\Omega \in \mathbb{R}^{G \times d}$ is constructed as follows:

\textbf{Input:} Number of genes $G$, sketch dimension $d$, leverage scores $\ell \in \mathbb{R}^G$

\textbf{Algorithm:}
\begin{enumerate}
    \item Normalize leverage scores: $p_g = \ell_g / \sum_{g'} \ell_{g'}$ (probability distribution)
    \item For each gene $g \in \{1, \ldots, G\}$:
    \begin{enumerate}
        \item Hash assignment: $h(g) \sim \text{Uniform}(\{1, \ldots, d\})$
        \item Sign assignment: $s(g) \sim \text{Uniform}(\{-1, +1\})$
        \item Importance weight: $w_g = \sqrt{p_g \cdot G}$ (scaled by leverage)
        \item Clip weight: $w_g = \text{clip}(w_g, 0.1, 10.0)$ (numerical stability)
    \end{enumerate}
    \item Construct sparse matrix: $\Omega[g, h(g)] = s(g) \cdot w_g$
    \item Column normalization: For each column $j$:
    \begin{equation}
    \Omega[:, j] \leftarrow \Omega[:, j] \cdot \frac{\sqrt{G/d}}{\|\Omega[:, j]\|_2}
    \end{equation}
\end{enumerate}

\textbf{Output:} Sparse matrix $\Omega$ of shape $(G, d)$ with exactly $G$ non-zero entries

The key innovation is the \textbf{importance weighting} $w_g = \sqrt{p_g \cdot G}$: genes with higher leverage scores contribute more strongly to the sketch, preserving discriminative information for rare cell types.

\subsubsection{Leverage Score Computation}

Leverage scores quantify the importance of each gene for distinguishing cell types. We compute them via SVD of the reference matrix:
\begin{equation}
X^T = U \Sigma V^T
\end{equation}
where $X \in \mathbb{R}^{K \times G}$ is the (centered) reference signature matrix.

The leverage score for gene $g$ is:
\begin{equation}
\ell_g = \sum_{j=1}^{r} \frac{\sigma_j^2}{\sigma_j^2 + \epsilon} \cdot U_{gj}^2
\end{equation}
where $r = \min(K, G)$ is the rank, $\sigma_j$ are singular values, and $\epsilon = 10^{-6}$ prevents division by zero. This weighted sum emphasizes genes that lie in the principal subspace of the cell type signatures.

\textbf{Intuition:} Genes with high leverage scores are those that:
\begin{itemize}
    \item Have high variance across cell types (captured by large $\sigma_j$)
    \item Are not redundant with other genes (unique directions in $U$)
    \item Are essential for distinguishing rare cell types (marker genes)
\end{itemize}

\subsubsection{Integration: CountSketch + Importance Sampling}

The combination works as follows:

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Standard CountSketch} & \textbf{Our Method} \\ \midrule
Hash function & Uniform random & Uniform random \\
Sign function & $\pm 1$ uniform & $\pm 1$ uniform \\
Entry value & $\pm 1$ & $\pm w_g$ (leverage-weighted) \\
Expectation & $\mathbb{E}[\Omega^T\Omega] = I$ & $\mathbb{E}[\Omega^T\Omega] \approx I$ (weighted) \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Theoretical Guarantee (Leverage Score Sampling Framework):}

Following the leverage score sampling framework (Drineas et al., 2006; Mahoney, 2011), the weighted sketch preserves the column space of $X$ with high probability. Specifically, for the regression problem $\min_\beta \|Y - \beta X\|_F^2$, solving in the sketched space yields a $(1+\epsilon)$-approximation:
\begin{equation}
\|\tilde{Y} - \hat{\beta} \tilde{X}\|_F^2 \leq (1 + \epsilon) \min_\beta \|\tilde{Y} - \beta \tilde{X}\|_F^2
\end{equation}
with probability at least $1 - \delta$, where $d = O(K \log(K) / \epsilon^2)$ suffices.

The importance weighting ensures that the effective approximation error is \textbf{lower for high-leverage genes} (cell type markers), preserving rare cell type signals even after dimension reduction. Unlike standard Johnson-Lindenstrauss projections, this leverage-based approach provides \textbf{non-uniform preservation} tailored to the regression structure.

We then project the data into the low-dimensional ``sketch space'':
\begin{equation}
Y_{sketch} = \tilde{Y} \Omega \quad (N \times d)
\end{equation}
\begin{equation}
X_{sketch} = \tilde{X} \Omega \quad (K \times d)
\end{equation}

\textbf{Theoretical Guarantee:} According to the Johnson-Lindenstrauss lemma and recent extensions for oblivious subspace embeddings, solving the regression in this $d$-dimensional sketch space yields a solution $\hat{\beta}$ that is an $(1+\epsilon)$-approximation of the optimal solution in the original space, with high probability.

\subsection{Spatial Graph Laplacian Regularization}

To incorporate spatial information (inspired by CARD) without the computational burden of inverting dense covariance matrices, we introduce a \textbf{Graph Laplacian Regularizer}.

We construct a spatial neighbor graph adjacency matrix $A$, where $A_{ij}=1$ if spot $i$ and $j$ are physical neighbors, else 0. The Laplacian matrix is $L = D - A$, where $D$ is the degree matrix.

Our final objective function in the sketch space is:
\begin{equation}
\mathcal{L}(\beta) = \underbrace{\frac{1}{2} \| Y_{sketch} - \beta X_{sketch} \|_F^2}_{\text{Fidelity in Sketch Domain}} + \underbrace{\lambda \cdot \text{Tr}(\beta^T L \beta)}_{\text{Spatial Smoothing}} + \underbrace{\rho \|\beta\|_1}_{\text{Sparsity}}
\end{equation}
Subject to: $\beta \geq 0$.

The term $\text{Tr}(\beta^T L \beta) = \sum_{(i,j)\in E} \|\beta_i - \beta_j\|^2$ forces neighboring spots to have similar cell type compositions, effectively denoising ``drop-out'' events in low-coverage spots.

\subsection{Fast Optimization: Block Coordinate Descent}

While the objective function is convex, standard gradient descent is slow for large $N$. We derive a \textbf{Block Coordinate Descent (BCD)} algorithm that solves row-by-row (spot-by-spot) but leverages shared pre-computed matrices.

For each spot $i$, the update rule for cell type $k$ ($\beta_{ik}$) has a \textbf{closed-form solution} combining soft-thresholding (for L1) and projection to non-negative (for constraints).

Let $G = X_{sketch} X_{sketch}^T$ (the Gram matrix, precomputed once). The update is:
\begin{equation}
r_{ik} = (Y_{sketch})_i (X_{sketch})_k^T - \sum_{j \neq k} \beta_{ij} G_{jk} + \lambda \sum_{n \in \mathcal{N}(i)} \beta_{nk}
\end{equation}
\begin{equation}
\beta_{ik} \leftarrow \left[ \frac{S_\rho(r_{ik})}{G_{kk} + \lambda |\mathcal{N}(i)|} \right]_+
\end{equation}
where $S_\rho(x) = \text{sign}(x) \cdot \max(0, |x| - \rho)$ is the \textbf{soft-thresholding operator} for L1 regularization, and $[\cdot]_+ = \max(0, \cdot)$ enforces non-negativity.

\textbf{Convergence Criteria:}
\begin{itemize}
    \item \textbf{Maximum iterations}: 100 (default)
    \item \textbf{Tolerance}: $\text{rel\_change} = \frac{\max|\beta^{(t)} - \beta^{(t-1)}|}{\max|\beta^{(t-1)}| + 10^{-10}} < 10^{-4}$
    \item Empirically, convergence is achieved in \textbf{10-30 iterations} for most datasets.
\end{itemize}

\textbf{Computational Edge:}
\begin{itemize}
    \item The term $(X_{sketch})^T X_{sketch}$ is a tiny $K \times K$ matrix computed only once.
    \item The neighbor sum $\sum_{n \in \mathcal{N}(i)} \beta_{nk}$ exploits the sparsity of the spatial graph.
    \item The algorithm converges in very few iterations and is fully parallelizable across spots.
\end{itemize}

\subsection{Complexity Analysis}

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Component} & \textbf{Cell2location (VI)} & \textbf{CARD (CAR)} & \textbf{FlashDeconv (Ours)} \\ \midrule
Data Representation & Dense $N \times G$ & Dense $N \times G$ & Sketch $N \times d$ \\
Spatial Model & N/A (or slow MRF) & Dense Matrix Inversion & Sparse Laplacian \\
Time Complexity & $O(Iter \cdot N \cdot G \cdot K)$ & $O(N^3)$ (approx) & $O(Iter \cdot N \cdot d \cdot K)$ \\
Memory Usage & High (GBs to TBs) & High ($N \times N$ matrix) & \textbf{Ultra-Low} ($O(Nd)$) \\
1M Spots Time & $>$ 24 Hours (GPU) & Out of Memory & \textbf{$\sim$ 10 Minutes (CPU)} \\ \bottomrule
\end{tabular}
\caption{Computational complexity comparison of different deconvolution methods.}
\end{table}

\subsection{Implementation Details for Reproducibility}

\begin{itemize}
    \item \textbf{Software:} Implemented in Python using \texttt{Numba} for JIT compilation of the BCD solver.
    \item \textbf{Hardware Independence:} Unlike Tangram or Cell2location, FlashDeconv requires \textbf{no GPU} and runs efficiently on standard laptops (e.g., MacBook Air with 8GB RAM).
\end{itemize}

\textbf{Default Hyperparameters:}

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Parameter} & \textbf{Default} & \textbf{Valid Range} & \textbf{Description} \\ \midrule
$d$ (sketch\_dim) & 512 & [128, 2048] & Sketch dimension \\
$\lambda$ (lambda\_spatial) & 5000 or ``auto'' & $[0, \infty)$ & Spatial smoothing; 0 disables \\
$\rho$ (rho\_sparsity) & 0.01 & [0, 0.1] & L1 regularization for sparsity \\
n\_hvg & 2000 & [500, 5000] & Number of highly variable genes \\
n\_markers\_per\_type & 50 & [20, 200] & Marker genes per cell type \\
k\_neighbors & 6 & [4, 20] & Neighbors for spatial graph \\
preprocess & ``log\_cpm'' & \{log\_cpm, pearson, raw\} & Preprocessing method \\
max\_iter & 100 & -- & Maximum BCD iterations \\
tol & $10^{-4}$ & -- & Convergence tolerance \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Automatic $\lambda$ Tuning:}

When \texttt{lambda\_spatial="auto"}, we estimate $\lambda$ by balancing the data fidelity term and the spatial regularization term. The key insight is that $\lambda$ must be scaled relative to the Gram matrix $X_{sketch}^T X_{sketch}$ to have meaningful effect on the optimization.

In the BCD update, the denominator is:
\begin{equation}
\text{denom} = (X_{sketch}^T X_{sketch})_{kk} + \lambda \cdot n_{neighbors}
\end{equation}

For spatial regularization to contribute meaningfully, we set:
\begin{equation}
\lambda = \alpha \cdot \frac{\text{mean}(\text{diag}(X_{sketch}^T X_{sketch}))}{\bar{d}}
\end{equation}
where:
\begin{itemize}
    \item $\alpha = 0.005$ (default) controls the relative strength of spatial regularization
    \item $\bar{d}$ is the average number of neighbors per spot
\end{itemize}

This ensures that the spatial term contributes approximately $\alpha / (1 + \alpha) \approx 0.5\%$ to the Hessian diagonal. Users can adjust the effective regularization strength by setting a fixed \texttt{lambda\_spatial} value (recommended range: 1000-10000 for Visium data).

\textbf{Numerical Stability Clipping:}

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Quantity} & \textbf{Clip Range} & \textbf{Rationale} \\ \midrule
Sketch weights & [0.1, 10] & Balanced importance sampling \\
Leverage scores & Normalized to sum=1 & Proper probability distribution \\ \bottomrule
\end{tabular}
\end{table}

\section*{Key Design Decisions}

\begin{enumerate}
    \item \textbf{Flexible Preprocessing:} Unlike Cell2location/RCTD which require specific data formats, FlashDeconv supports multiple preprocessing modes (\texttt{log\_cpm}, \texttt{pearson}, \texttt{raw}) to accommodate different data types and user preferences.
    \item \textbf{Spatial Regularization via Graph Laplacian:} Inspired by CARD's spatial modeling, but using sparse Graph Laplacian instead of dense CAR matrix inversion. This provides $O(N \cdot K)$ complexity instead of $O(N^3)$.
    \item \textbf{Structure-Preserving Sketching:} Leverage-weighted CountSketch ensures rare cell type markers are preserved during dimension reduction, addressing the ``random projection loses rare cells'' concern.
    \item \textbf{CPU-only, Memory-efficient:} No GPU required. The algorithm runs efficiently on standard laptops (8GB RAM), making it accessible for Stereo-seq/Xenium users with million-scale datasets.
    \item \textbf{Parameter Sensitivity:} The \texttt{lambda\_spatial} parameter may require tuning for different data types:
    \begin{itemize}
        \item For sequencing-based platforms (10x Visium): default 5000 works well
        \item For imaging-based platforms (seqFISH+, MERFISH): consider \texttt{lambda\_spatial=0} to disable spatial regularization when sample sizes are small
    \end{itemize}
\end{enumerate}

\end{document}
