# FlashDeconv Sparse Matrix Optimization Plan

## Executive Summary

å½“å‰ FlashDeconv ä»£ç åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶å­˜åœ¨å†…å­˜ç“¶é¢ˆï¼Œä¸»è¦åŸå› æ˜¯åœ¨å¤šä¸ªä½ç½®å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºç¨ å¯†çŸ©é˜µã€‚æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æé—®é¢˜å¹¶æå‡ºä¿®å¤æ–¹æ¡ˆã€‚

---

## 1. é—®é¢˜è¯Šæ–­

### 1.1 å†…å­˜ç“¶é¢ˆä½ç½®

| ä½ç½® | æ–‡ä»¶:è¡Œå· | é—®é¢˜ä»£ç  | ä¸¥é‡æ€§ |
|------|----------|----------|--------|
| **P1** | `genes.py:46-47` | `Y_dense = Y.toarray()` | ğŸ”´ è‡´å‘½ |
| **P2** | `deconv.py:240-241` | `Y_subset = Y[:, gene_idx].toarray()` | ğŸŸ¡ ä¸¥é‡ |
| **P3** | `deconv.py:144` | `Y / Y.sum(axis=1, keepdims=True)` | ğŸŸ¡ ä¸¥é‡ |

### 1.2 å†…å­˜æ¶ˆè€—ä¼°ç®—

å‡è®¾ `N = 1,000,000` spots, `G = 30,000` genes, `G' = 4,000` selected genes:

| é—®é¢˜ | çŸ©é˜µç»´åº¦ | å†…å­˜ (float64) |
|------|----------|----------------|
| P1: å…¨çŸ©é˜µè½¬ dense | N Ã— G | **240 GB** |
| P2: å­é›†è½¬ dense | N Ã— G' | **32 GB** |
| P3: numpy é™¤æ³•è¾“å‡º | N Ã— G' | **32 GB** |
| ç†æƒ³: Sketch è¾“å‡º | N Ã— 512 | **4 GB** |

### 1.3 å½“å‰æ•°æ®æµ

```
Y_sparse (N Ã— 30K, sparse)
    â”‚
    â”œâ”€â”€â–¶ select_hvg()
    â”‚       â””â”€â”€ Y.toarray()           â† P1: 240GB å†…å­˜çˆ†ç‚¸ï¼
    â”‚
    â”œâ”€â”€â–¶ gene_idx (2000-4000 genes)
    â”‚
    â–¼
Y[:, gene_idx].toarray()              â† P2: 32GB
    â”‚
    â–¼
_preprocess_data()
    â”‚   Y / Y.sum(axis=1) * 1e4       â† P3: numpy å¹¿æ’­äº§ç”Ÿ dense
    â”‚   np.log1p(Y_cpm)
    â–¼
Y_tilde (dense, N Ã— 4K)
    â”‚
    â–¼
sketch_data()
    â”‚   Y_tilde @ Omega
    â–¼
Y_sketch (dense, N Ã— 512)             â† è¿™ä¸€æ­¥æ˜¯åˆç†çš„
```

---

## 2. ä¿®å¤æ–¹æ¡ˆ

### 2.1 ç›®æ ‡æ•°æ®æµ

```
Y_sparse (N Ã— 30K, sparse CSR)
    â”‚
    â”œâ”€â”€â–¶ select_hvg_sparse()          â† å…¨ç¨‹ sparse æ“ä½œ
    â”‚       â””â”€â”€ mean, var ç›´æ¥ä» sparse è®¡ç®—
    â”‚
    â”œâ”€â”€â–¶ gene_idx
    â”‚
    â–¼
Y_sparse[:, gene_idx]                 â† ä¿æŒ sparse (N Ã— 4K)
    â”‚
    â–¼
_preprocess_data_sparse()
    â”‚   diags(1/lib_size) @ Y         â† sparse @ sparse = sparse
    â”‚   Y.data = np.log1p(Y.data)     â† åŸä½æ“ä½œï¼Œä¿æŒç¨€ç–
    â–¼
Y_tilde (sparse, N Ã— 4K)
    â”‚
    â–¼
sketch_data()
    â”‚   Y_sparse @ Omega_dense        â† sparse @ dense = dense (é«˜æ•ˆ)
    â–¼
Y_sketch (dense, N Ã— 512)             â† æœ€ç»ˆè¾“å‡ºï¼Œå¯æ¥å—
```

---

## 3. å…·ä½“ä¿®æ”¹

### 3.1 ä¿®æ”¹ P1: `genes.py` - HVG é€‰æ‹©

**å½“å‰ä»£ç  (lines 45-49):**
```python
# Convert to dense if sparse
if sparse.issparse(Y):
    Y_dense = Y.toarray()  # âŒ 240GB for 1M spots
else:
    Y_dense = np.asarray(Y)
```

**ä¿®æ”¹æ–¹æ¡ˆ (æ•°å€¼å®Œå…¨ç­‰ä»·):**

**å…³é”®æ´å¯Ÿ: `log1p(0) = 0`ï¼Œæ‰€ä»¥ log å˜æ¢åç¨€ç–ç»“æ„ä¿æŒä¸å˜ï¼**

```python
def select_hvg_sparse(Y, n_top=2000):
    """
    Sparse-friendly HVG selection - NUMERICALLY EQUIVALENT to dense version.

    Key insight: log1p(0) = 0, so sparsity is preserved after log transform!
    """
    N, G = Y.shape

    if sparse.issparse(Y):
        # Step 1: Row normalize (CPM-like) using diagonal matrix
        lib_size = np.array(Y.sum(axis=1)).flatten()
        lib_size = np.maximum(lib_size, 1.0)
        D = diags(10000.0 / lib_size)
        Y_norm = D @ Y  # Still sparse!

        # Step 2: Log1p transform - zeros stay zeros!
        Y_log = Y_norm.copy()
        Y_log.data = np.log1p(Y_log.data)

        # Step 3: Compute mean and variance per gene
        gene_means = np.array(Y_log.mean(axis=0)).flatten()
        mean_sq = np.array(Y_log.power(2).mean(axis=0)).flatten()
        # Sample variance (ddof=1): Var = N/(N-1) * (E[X^2] - E[X]^2)
        gene_vars = N / (N - 1) * (mean_sq - gene_means ** 2)
        gene_vars = np.maximum(gene_vars, 0)

        # Step 4: Bin-based normalization (same as original, on small arrays)
        n_bins = 20
        bins = np.percentile(gene_means[gene_means > 0], np.linspace(0, 100, n_bins + 1))
        bins = np.unique(bins)

        gene_bins = np.digitize(gene_means, bins) - 1
        gene_bins = np.clip(gene_bins, 0, len(bins) - 2)

        normalized_dispersion = np.zeros(G)
        for i in range(len(bins) - 1):
            mask = gene_bins == i
            if np.sum(mask) > 1:
                bin_vars = gene_vars[mask]
                bin_mean = np.mean(bin_vars)
                bin_std = np.std(bin_vars) + 1e-10
                normalized_dispersion[mask] = (bin_vars - bin_mean) / bin_std

        # Step 5: Select top genes
        hvg_idx = np.argsort(normalized_dispersion)[::-1][:n_top]
        return np.sort(hvg_idx)
    else:
        # Dense path (original logic)
        ...
```

**éªŒè¯ç»“æœ (è§ `validation/sparse_hvg_correctness_test.py`):**
- HVG é‡å ç‡: **100%** (2000/2000)
- å‡å€¼å·®å¼‚: 9.99e-16
- æ–¹å·®å·®å¼‚: 6.11e-14
- å†…å­˜å‡å°‘: **4.2x**
- é€Ÿåº¦æå‡: **4.0x**

### 3.2 ä¿®æ”¹ P2: `deconv.py` - åˆ é™¤ toarray()

**å½“å‰ä»£ç  (lines 239-244):**
```python
# Subset to selected genes
if sparse.issparse(Y):
    Y_subset = Y[:, gene_idx].toarray()  # âŒ 32GB
else:
    Y_subset = Y[:, gene_idx]
X_subset = X[:, gene_idx]
```

**ä¿®æ”¹æ–¹æ¡ˆ:**
```python
# Subset to selected genes (keep sparse!)
if sparse.issparse(Y):
    Y_subset = Y[:, gene_idx]  # âœ“ Still CSR, N Ã— 4000
    # Ensure CSR format for efficient row operations
    if not sparse.isspmatrix_csr(Y_subset):
        Y_subset = Y_subset.tocsr()
else:
    Y_subset = Y[:, gene_idx]
X_subset = X[:, gene_idx]
```

### 3.3 ä¿®æ”¹ P3: `deconv.py` - Sparse Log-CPM

**å½“å‰ä»£ç  (lines 142-146):**
```python
if method == "log_cpm":
    # CPM normalization + log1p (recommended)
    Y_cpm = Y / (Y.sum(axis=1, keepdims=True) + 1e-10) * 1e4  # âŒ dense output
    X_cpm = X / (X.sum(axis=1, keepdims=True) + 1e-10) * 1e4
    return np.log1p(Y_cpm), np.log1p(X_cpm)
```

**ä¿®æ”¹æ–¹æ¡ˆ:**
```python
from scipy.sparse import diags, issparse

def _preprocess_data(self, Y, X, method):
    if method == "log_cpm":
        # ========== Y (å¯èƒ½æ˜¯ sparse) ==========
        if issparse(Y):
            # 1. Compute library size (row sums)
            lib_size = np.array(Y.sum(axis=1)).flatten()
            lib_size[lib_size == 0] = 1.0

            # 2. Build diagonal scaling matrix D = diag(1e4 / lib_size)
            scale_factors = 1e4 / lib_size
            D = diags(scale_factors)

            # 3. CPM = D @ Y (sparse @ sparse = sparse)
            Y_cpm = D @ Y

            # 4. Log1p: operate on .data directly (preserves sparsity!)
            # Because log1p(0) = 0, zeros stay zeros
            Y_norm = Y_cpm.copy()
            Y_norm.data = np.log1p(Y_norm.data)
        else:
            # Dense path
            lib_size = Y.sum(axis=1, keepdims=True)
            lib_size[lib_size == 0] = 1.0
            Y_cpm = Y / lib_size * 1e4
            Y_norm = np.log1p(Y_cpm)

        # ========== X (é€šå¸¸å¾ˆå°ï¼Œdense æ²¡é—®é¢˜) ==========
        if issparse(X):
            X = X.toarray()  # K Ã— G' is small (~800KB)
        X_lib = X.sum(axis=1, keepdims=True)
        X_lib[X_lib == 0] = 1.0
        X_cpm = X / X_lib * 1e4
        X_norm = np.log1p(X_cpm)

        return Y_norm, X_norm
```

### 3.4 ç¡®è®¤ Sketching å…¼å®¹æ€§

**å½“å‰ä»£ç  (`sketching.py:204`):**
```python
Y_sketch = Y_tilde @ Omega
```

**åˆ†æ:**
- å¦‚æœ `Y_tilde` æ˜¯ sparse CSR, `Omega` æ˜¯ dense ndarray
- `scipy.sparse` çš„ `@` è¿ç®—ç¬¦ä¼šè‡ªåŠ¨è°ƒç”¨ä¼˜åŒ–çš„ sparse-dense ä¹˜æ³•
- è¾“å‡ºæ˜¯ dense ndarray (N Ã— d)ï¼Œè¿™æ˜¯æˆ‘ä»¬æœŸæœ›çš„

**å»ºè®®:** ç¡®ä¿ `Omega` æ˜¯ dense ndarrayï¼ˆè€Œä¸æ˜¯ sparseï¼‰ï¼š
```python
def sketch_data(Y_tilde, X_tilde, Omega, ...):
    # Ensure Omega is dense for optimal sparse @ dense multiplication
    if sparse.issparse(Omega):
        Omega = Omega.toarray()

    # This handles both sparse and dense Y_tilde
    Y_sketch = Y_tilde @ Omega  # âœ“ Works for both
    X_sketch = X_tilde @ Omega

    return Y_sketch, X_sketch
```

---

## 4. å®ç°æ£€æŸ¥æ¸…å•

- [ ] **P1: `genes.py:select_hvg()`**
  - [ ] ä½¿ç”¨ `Y.mean(axis=0)` ä»£æ›¿ `np.mean(Y_dense, axis=0)`
  - [ ] ä½¿ç”¨ `Y.power(2).mean(axis=0)` è®¡ç®— E[YÂ²]
  - [ ] ç§»é™¤ `Y.toarray()` è°ƒç”¨
  - [ ] éªŒè¯: 1M spots æ—¶å†…å­˜ä¸è¶…è¿‡è¾“å…¥çŸ©é˜µå¤§å°

- [ ] **P2: `deconv.py:fit()` line 240**
  - [ ] ç§»é™¤ `.toarray()` è°ƒç”¨
  - [ ] ç¡®ä¿è¿”å› CSR æ ¼å¼

- [ ] **P3: `deconv.py:_preprocess_data()`**
  - [ ] å¯¹ sparse Y ä½¿ç”¨ `diags()` è¿›è¡Œè¡Œå½’ä¸€åŒ–
  - [ ] ä½¿ç”¨ `Y.data = np.log1p(Y.data)` åŸä½æ“ä½œ
  - [ ] ä¿æŒ X çš„ dense å¤„ç†ï¼ˆå› ä¸º K å¾ˆå°ï¼‰

- [ ] **P4: `sketching.py:sketch_data()`**
  - [ ] ç¡®ä¿ Omega æ˜¯ dense
  - [ ] éªŒè¯ sparse @ dense å·¥ä½œæ­£å¸¸

---

## 5. éªŒè¯æ–¹æ¡ˆ

### 5.1 å•å…ƒæµ‹è¯•

æ¯ä¸ªä¿®æ”¹ç‚¹éƒ½éœ€è¦éªŒè¯ï¼š
1. **æ­£ç¡®æ€§**: è¾“å‡ºä¸åŸ dense å®ç°ä¸€è‡´ï¼ˆå…è®¸æµ®ç‚¹è¯¯å·® < 1e-6ï¼‰
2. **ç¨€ç–æ€§**: ä¸­é—´ç»“æœä¿æŒ sparse
3. **å†…å­˜**: å³°å€¼å†…å­˜ç¬¦åˆé¢„æœŸ

### 5.2 é›†æˆæµ‹è¯•

```python
# æµ‹è¯•åœºæ™¯
test_cases = [
    {"N": 5000, "G": 20000, "expected_peak_mb": 500},    # Visium
    {"N": 100000, "G": 20000, "expected_peak_mb": 5000}, # Xenium
    {"N": 1000000, "G": 20000, "expected_peak_mb": 12000}, # Atlas (target!)
]
```

### 5.3 Benchmark è„šæœ¬

è§ `validation/sparse_benchmark.py`

---

## 6. éªŒè¯ç»“æœ

### 6.1 Demo è„šæœ¬éªŒè¯

**Log-CPM éªŒè¯** (`validation/sparse_optimization_demo.py --demo logcpm`):
```
Sparse Log-CPM: 178.6 MB, 0.065s
Dense Log-CPM:  640.1 MB, 0.308s
Memory reduction: 3.6x
Max difference: 8.88e-16 (numerically identical)
```

**HVG é€‰æ‹©éªŒè¯** (`validation/sparse_hvg_correctness_test.py`):
```
N=5000, G=10000, density=0.15, n_top=2000

Numerical Comparison:
  Gene means max diff: 9.99e-16
  Gene vars max diff:  6.11e-14
  Norm dispersion max diff: 9.00e-12

HVG Selection:
  Overlap: 2000/2000 (100.0%)  â† å®Œå…¨ä¸€è‡´ï¼
  Identical HVG sets: True

Memory/Speed (N=20000, G=20000):
  Sparse: 2285 MB, 0.92s
  Dense:  9601 MB, 3.69s
  Reduction: 4.2x memory, 4.0x faster
```

### 6.2 å®Œæ•´ Pipeline Benchmark

è¿è¡Œ `validation/sparse_memory_benchmark.py`:

| Scale | N | Current Peak | Optimized Peak | Reduction |
|-------|---|--------------|----------------|-----------|
| Small | 5,000 | 1,837 MB | 368 MB | **5.0x** |
| Medium | 50,000 | 19,332 MB | 3,779 MB | **5.1x** |
| Large | 200,000 | ~32,000 MB (OOM) | 9,847 MB | **3.3x** |

### 6.3 å…³é”®å‘ç°

1. **ä¼˜åŒ–ç‰ˆæœ¬åœ¨ 200K spots ä»…éœ€ ~10GB å†…å­˜**ï¼ˆå½“å‰ç‰ˆæœ¬éœ€è¦ 32GBï¼Œä¼š OOMï¼‰
2. **æ‰€æœ‰é˜¶æ®µéƒ½æœ‰æ˜¾è‘—æ”¹å–„**ï¼ŒHVG é˜¶æ®µæ”¹å–„æœ€å¤§
3. **æ•°å€¼ç²¾åº¦å®Œå…¨ä¿æŒ**ï¼š
   - Log-CPM å·®å¼‚ < 1e-15
   - HVG é€‰æ‹© **100% ä¸€è‡´**ï¼ˆå…³é”®æ´å¯Ÿï¼š`log1p(0) = 0`ï¼‰
4. **é€Ÿåº¦æå‡ 4x**ï¼šsparse æ“ä½œæ¯” dense æ›´å¿«

---

## 7. é£é™©è¯„ä¼°

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| æ•°å€¼ç²¾åº¦å·®å¼‚ | ç»“æœç•¥æœ‰ä¸åŒ | è®¾ç½®åˆç†çš„ tolerance |
| scipy ç‰ˆæœ¬å…¼å®¹ | æ—§ç‰ˆæœ¬å¯èƒ½ç¼ºå°‘æŸäº› API | æ–‡æ¡£è¯´æ˜æœ€ä½ç‰ˆæœ¬è¦æ±‚ |
| Pearson residuals ä¸å…¼å®¹ | è¯¥æ–¹æ³•éœ€è¦å…¨çŸ©é˜µæ“ä½œ | ä»…å¯¹ log_cpm åšä¼˜åŒ– |

---

## 7. å‚è€ƒèµ„æ–™

- [scipy.sparse documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html)
- [Efficient sparse matrix operations (Stack Overflow)](https://stackoverflow.com/questions/12305021/efficient-way-to-normalize-a-scipy-sparse-matrix)
- [scanpy sparse handling](https://scanpy.readthedocs.io/)
